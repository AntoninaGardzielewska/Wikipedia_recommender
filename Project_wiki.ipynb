{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fae359a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Desktop\\studia\\sem5\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Any, Literal\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from numpy.typing import NDArray\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "from transformers import AutoFeatureExtractor, AutoModel\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "wordnet = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed4b90c",
   "metadata": {},
   "source": [
    "# Main processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18caf320",
   "metadata": {},
   "source": [
    "## Crawling and scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab81548",
   "metadata": {},
   "source": [
    "run the spider:\n",
    "\n",
    "scrapy runspider my_spider.py \n",
    "additional parameters:\n",
    "* -a max_links # maximum number of links to collect\n",
    "* -a start_url # starting URL\n",
    "* -a min_word_count # minimum word count in text to be saved\n",
    "* -a allow_random # whether to fetch random articles when stuck\n",
    "* -a source_type # 'wiki' or 'fandom' to restrict to one source when fetching random articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208964e8",
   "metadata": {},
   "source": [
    "we already cutted out most of the characters.\n",
    "* we filtered wikipedia articles so there are only correct ones\n",
    "* we left only letters and numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59adc6fb",
   "metadata": {},
   "source": [
    "## Stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bb50457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove stopwords and non-alphabetic tokens\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    \n",
    "    cleaned_text = ' '.join(filtered_tokens)\n",
    "    return cleaned_text\n",
    "\n",
    "def stem_lemmatize(text, technique: str):\n",
    "    words = word_tokenize(text.lower())\n",
    "    if technique == 'PorterStemmer':\n",
    "        processed = [porter.stem(w) for w in words]\n",
    "    elif technique == 'LancasterStemmer':\n",
    "        processed = [lancaster.stem(w) for w in words]\n",
    "    elif technique == 'WordNetLemmatizer':\n",
    "        processed = [wordnet.lemmatize(w) for w in words]\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid technique: {technique}. Choose 'PorterStemmer', 'LancasterStemmer', or 'WordNetLemmatizer'.\")\n",
    "    return ' '.join(processed)\n",
    "\n",
    "def process_all_articles():\n",
    "    df_fandom = pd.read_csv('data/fandom.csv', encoding='utf-8')\n",
    "    df_wiki = pd.read_csv('data/wiki.csv', encoding='utf-8')\n",
    "    df_combined = pd.concat([df_wiki, df_fandom], ignore_index=True)\n",
    "    df = df_combined.drop_duplicates().reset_index(drop = True)\n",
    "    print('Data read from CSV files')\n",
    "    \n",
    "    print('Running data preprocessing, stemminization and lemmatization')\n",
    "    # tokenization, lowercasing, stopword removal\n",
    "    df['text_processed'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "    # stemming and lemmatization\n",
    "    df['wordnet_lemmatized'] = df['text_processed'].apply(lambda x: stem_lemmatize(x, technique='WordNetLemmatizer'))\n",
    "\n",
    "    df.to_csv('data/processed_data.csv', index=False, encoding='utf-8')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d35ffda",
   "metadata": {},
   "source": [
    "## Simple Spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70d1cae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_article(url):\n",
    "    parsed = urlparse(url)\n",
    "    if parsed.scheme not in ('http', 'https'):\n",
    "        return False\n",
    "\n",
    "    invalid_prefixes = (\n",
    "        '/wiki/wikipedia:',\n",
    "        '/wiki/help:',\n",
    "        '/wiki/talk:',\n",
    "        '/wiki/file:',\n",
    "        '/wiki/category:',\n",
    "        '/wiki/template:',\n",
    "        '/wiki/portal:',\n",
    "        '/wiki/special:',\n",
    "        '/wiki/draft:',\n",
    "        '/wiki/module:',\n",
    "        '/wiki/user:',\n",
    "        '/wiki/forum:',\n",
    "        '/wiki/message_wall:'\n",
    "    )\n",
    "\n",
    "    path = parsed.path.lower()\n",
    "    return not path.startswith(invalid_prefixes)\n",
    "\n",
    "def fetch_article(url, min_word_count=10):\n",
    "    r = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    text = ' '.join([p.get_text() for p in soup.find_all('p')])\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    if len(text.split()) < min_word_count:\n",
    "        return None\n",
    "    return {'url': url, 'text': text}\n",
    "\n",
    "def run_scraper_simple(user_urls, min_word_count=10):\n",
    "    for url in user_urls:\n",
    "        if not is_article(url):\n",
    "            print(f\"Skipping non-article URL: {url}\")\n",
    "\n",
    "    data = [fetch_article(url) for url in user_urls]\n",
    "    data = [d for d in data if d is not None]\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    df.to_csv('data/user_articles.csv', index=False, encoding='utf8')\n",
    "    print(\"Scraping completed. Data saved to 'data/user_articles.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f515eadc",
   "metadata": {},
   "source": [
    "## Processing user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94becf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- find top_n most diverse articles from user input ---\n",
    "def most_diverse_articles(embeddings, top_n=100):\n",
    "    # 1. Cluster into top_n groups\n",
    "    kmeans = KMeans(n_clusters=top_n, random_state=42)\n",
    "    kmeans.fit(embeddings)\n",
    "\n",
    "    # 2. Get the index of the article closest to each cluster center\n",
    "    diverse_indices = []\n",
    "    for i in range(top_n):\n",
    "        cluster_indices = np.where(kmeans.labels_ == i)[0]\n",
    "        centroid = kmeans.cluster_centers_[i]\n",
    "        cluster_embeds = embeddings[cluster_indices]\n",
    "        distances = np.linalg.norm(cluster_embeds - centroid, axis=1)\n",
    "        closest_idx = cluster_indices[np.argmin(distances)]\n",
    "        diverse_indices.append(closest_idx)\n",
    "    return list(diverse_indices)  \n",
    "\n",
    "def process_user_articles(user_urls, \n",
    "                          max_user_articles: int = 100, \n",
    "                          min_word_count: int = 10):\n",
    "    run_scraper_simple(user_urls)\n",
    "    user_data = pd.read_csv('data/user_articles.csv', encoding='utf-8')\n",
    "\n",
    "    if len(user_data) == 0:\n",
    "        print(\"No valid articles were scraped from the provided URLs.\")\n",
    "        return None\n",
    "    \n",
    "    if len(user_data) > max_user_articles:\n",
    "        user_embeddings = SentenceTransformer('all-MiniLM-L6-v2').encode(user_data['text'])\n",
    "        diverse_indices = most_diverse_articles(user_embeddings, top_n=max_user_articles)\n",
    "        user_data = user_data.iloc[diverse_indices].reset_index(drop=True)\n",
    "        print(f\"User articles reduced to {max_user_articles} most diverse articles.\")\n",
    "\n",
    "    user_data['text_processed'] = user_data['text'].apply(preprocess_text) \n",
    "    user_data['wordnet_lemmatized'] = user_data['text_processed'].apply(lambda x: stem_lemmatize(x, technique='WordNetLemmatizer'))\n",
    "\n",
    "    return user_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79670db",
   "metadata": {},
   "source": [
    "## Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e38a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_recommended_articles(recommended_df, user_urls):\n",
    "    if 'url' not in recommended_df.columns:\n",
    "        raise ValueError(\"recommended_df must contain a 'url' column to filter by user URLs.\")\n",
    "    if not user_urls:\n",
    "        return recommended_df\n",
    "    filtered = recommended_df[~recommended_df['url'].isin(user_urls)].reset_index(drop=True)\n",
    "    return filtered\n",
    "\n",
    "# --- TF-IDF similarity ---\n",
    "def compute_tfidf_similarity(user_text: str, articles_texts: list, max_features: int = 10000):\n",
    "    # Computes cosine similarity between user text and article texts using TF-IDF.\n",
    "    vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "    tfidf_matrix = vectorizer.fit_transform(articles_texts.tolist() + [user_text])\n",
    "    article_tfidf = tfidf_matrix[:-1]\n",
    "    user_tfidf = tfidf_matrix[-1]\n",
    "    # print(f'user_text: {len(user_text)}; articles_texts: {articles_texts.shape}; tfidf: {tfidf_matrix.shape}, article_tfidf: {article_tfidf}')\n",
    "    similarity = cosine_similarity(article_tfidf, user_tfidf).flatten()\n",
    "    # print('tfidf:', similarity.shape)\n",
    "    return similarity\n",
    "\n",
    "# --- Embedding similarity ---\n",
    "def compute_embedding_similarity(user_texts: list, articles_texts: list):\n",
    "    # Computes cosine similarity using SentenceTransformer embeddings.\n",
    "    # If recency_weights=True, later articles are weighted higher.\n",
    "\n",
    "    embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    embedded_articles = embedder.encode(articles_texts, convert_to_tensor=True)\n",
    "\n",
    "    embedded_user = embedder.encode(user_texts, convert_to_tensor=True)\n",
    "    weights = np.linspace(1, 2, len(user_texts), dtype=np.float32)\n",
    "    weights_tensor = torch.tensor(weights).unsqueeze(1)\n",
    "    embedded_user = (embedded_user * weights_tensor).sum(dim=0) / weights_tensor.sum()\n",
    "\n",
    "    similarity = (embedded_articles @ embedded_user) / (\n",
    "        embedded_articles.norm(dim=1) * embedded_user.norm()\n",
    "    )\n",
    "    # print('embeddings: ', similarity.shape)\n",
    "    return similarity.cpu().numpy()\n",
    "\n",
    "def recommend_articles(user_df, \n",
    "                      articles_df, \n",
    "                      user_urls,\n",
    "                      min_similarity: float = 0.9,\n",
    "                      technique: str = 'tfidf',\n",
    "                      top_n: int = 10,\n",
    "                      tfidf_weight: float = 0.4,\n",
    "                      embedder_weight: float = 0.6):\n",
    "\n",
    "    if user_df is None or user_df.empty:\n",
    "        print(\"No user text available for recommendations.\")\n",
    "        return None\n",
    "\n",
    "    articles_df = articles_df.copy()\n",
    "\n",
    "    # --- User texts\n",
    "    user_texts = user_df['text']\n",
    "    user_wordnet_text = \" \".join(user_df['wordnet_lemmatized'].to_list()) # works as weighted average\n",
    "\n",
    "    # --- Article texts\n",
    "    article_texts = articles_df['text'] \n",
    "    article_wordnet_texts = articles_df['wordnet_lemmatized']\n",
    "\n",
    "    # --- Technique selection ---\n",
    "    final_sim = None\n",
    "    if technique == 'tfidf':\n",
    "        final_sim = compute_tfidf_similarity(user_wordnet_text, article_wordnet_texts)\n",
    "    elif technique == 'embeddings':\n",
    "        final_sim = compute_embedding_similarity(user_texts, article_texts)\n",
    "    elif technique == 'hybrid':\n",
    "        tfidf_sim = compute_tfidf_similarity(user_wordnet_text, article_wordnet_texts)\n",
    "        embedder_sim = compute_embedding_similarity(user_texts, article_texts)\n",
    "        # Normalize both similarity vectors before combining\n",
    "        tfidf_norm = (tfidf_sim - tfidf_sim.min()) / (tfidf_sim.max() - tfidf_sim.min() + 1e-9)\n",
    "        embedder_norm = (embedder_sim - embedder_sim.min()) / (embedder_sim.max() - embedder_sim.min() + 1e-9)\n",
    "        final_sim = tfidf_weight * tfidf_norm + embedder_weight * embedder_norm\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid technique: {technique}. Choose 'tfidf', 'embeddings', or 'hybrid'.\")\n",
    "\n",
    "    # --- Recommendation ---\n",
    "    articles_df['similarity'] = final_sim\n",
    "    # only articles that user did not see before\n",
    "    available_articles = filter_recommended_articles(articles_df, user_urls)\n",
    "    # Select articles above min_similarity\n",
    "    recommended = available_articles[available_articles['similarity'] >= min_similarity]\n",
    "\n",
    "    if len(recommended) == 0:\n",
    "        print(f'Could not find articles with required minimum similarity, returning top {top_n} most similar.')\n",
    "        recommended = available_articles.sort_values(by='similarity', ascending=False).head(top_n).reset_index(drop=True)\n",
    "    else:\n",
    "        recommended = recommended.sort_values(by='similarity', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return recommended\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138ac275",
   "metadata": {},
   "source": [
    "## Present predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "afbdf9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def present_predictions():\n",
    "    # TODO\n",
    "    # show some examples of recommendations with explanations (I'd prefer graphical form - see prediction breakdowns for example)\n",
    "    # ig jakaś prosta funkcja project_vector + wykres \n",
    "    # większość pewnie do przekopiowania z generate_embeddings\n",
    "    return NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "baddea5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# present interesting statistics about your database (most frequent words, histograms, similarities between documents, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8ca4b8",
   "metadata": {},
   "source": [
    "# Wikipedia recommender system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1d302ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = None\n",
    "try:\n",
    "    df = pd.read_csv('data/processed_data.csv', encoding='utf-8')\n",
    "except:\n",
    "    df = process_all_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "224fadd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 URLs from 'data/user_urls.txt'\n"
     ]
    }
   ],
   "source": [
    "# Initialize user_urls\n",
    "user_urls = []\n",
    "try:\n",
    "    # Try to open the file and read URLs\n",
    "    with open('data/user_urls.txt', 'r', encoding='utf-8') as f:\n",
    "        user_urls = [line.strip() for line in f if line.strip()]\n",
    "    print(f\"Loaded {len(user_urls)} URLs from 'data/user_urls.txt'\")\n",
    "except FileNotFoundError:\n",
    "    # File does not exist, ask user for input\n",
    "    print(\"File 'data/user_urls.txt' not found. Please enter URLs manually.\")\n",
    "    num_files = input(\"Enter number of URLs to input: \")\n",
    "    for i in range(int(num_files)):\n",
    "        url = input(f\"Enter URL {i+1}: \").strip()\n",
    "        if url:\n",
    "            user_urls.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b057e492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the following URLs: ['https://en.wikipedia.org/wiki/Monkey', 'https://en.wikipedia.org/wiki/Tiger']\n",
      "Scraping completed. Data saved to 'data/user_articles.csv'\n",
      "Could not find articles with required minimum similarity, returning top 10 most similar.\n",
      "╒═══════╤══════════════════════════════════════════════════════════════════════════════════════╤══════════╕\n",
      "│   No. │ title                                                                                │      URL │\n",
      "╞═══════╪══════════════════════════════════════════════════════════════════════════════════════╪══════════╡\n",
      "│     1 │ https://en.wikipedia.org/wiki/Monkeys_in_Chinese_culture                             │ 0.866419 │\n",
      "├───────┼──────────────────────────────────────────────────────────────────────────────────────┼──────────┤\n",
      "│     2 │ https://en.wikipedia.org/wiki/Platyrrhini                                            │ 0.822383 │\n",
      "├───────┼──────────────────────────────────────────────────────────────────────────────────────┼──────────┤\n",
      "│     3 │ https://en.wikipedia.org/wiki/Aotidae                                                │ 0.797528 │\n",
      "├───────┼──────────────────────────────────────────────────────────────────────────────────────┼──────────┤\n",
      "│     4 │ https://en.wikipedia.org/wiki/Howler_monkey                                          │ 0.79152  │\n",
      "├───────┼──────────────────────────────────────────────────────────────────────────────────────┼──────────┤\n",
      "│     5 │ https://en.wikipedia.org/wiki/Apes                                                   │ 0.730016 │\n",
      "├───────┼──────────────────────────────────────────────────────────────────────────────────────┼──────────┤\n",
      "│     6 │ https://en.wikipedia.org/wiki/Simians_(Chinese_poetry)                               │ 0.722337 │\n",
      "├───────┼──────────────────────────────────────────────────────────────────────────────────────┼──────────┤\n",
      "│     7 │ https://en.wikipedia.org/w/index.php?title=Simians_(Chinese_poetry)&oldid=1310610400 │ 0.722337 │\n",
      "├───────┼──────────────────────────────────────────────────────────────────────────────────────┼──────────┤\n",
      "│     8 │ https://en.wikipedia.org/wiki/Pitheciidae                                            │ 0.718461 │\n",
      "├───────┼──────────────────────────────────────────────────────────────────────────────────────┼──────────┤\n",
      "│     9 │ https://en.wikipedia.org/wiki/Colobinae                                              │ 0.694462 │\n",
      "├───────┼──────────────────────────────────────────────────────────────────────────────────────┼──────────┤\n",
      "│    10 │ https://en.wikipedia.org/wiki/Patasola_magdalenae                                    │ 0.680305 │\n",
      "╘═══════╧══════════════════════════════════════════════════════════════════════════════════════╧══════════╛\n"
     ]
    }
   ],
   "source": [
    "# Assumes that they are order by the time\n",
    "# Latest articles appears first\n",
    "# => Last article is the most recent one\n",
    "\n",
    "print(\"Processing the following URLs:\", user_urls)\n",
    "user_df = process_user_articles(user_urls, max_user_articles=5, min_word_count=10)\n",
    "\n",
    "r = recommend_articles(user_df, df, user_urls, technique='hybrid', top_n=10)\n",
    "# Add a numbering column\n",
    "r_display = r[['url', 'similarity']].reset_index(drop=True)\n",
    "r_display.index += 1  # start numbering from 1\n",
    "\n",
    "print(tabulate(r_display, headers=[\"No.\", \"title\", \"URL\", \"Similarity\"], tablefmt=\"fancy_grid\", showindex=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
